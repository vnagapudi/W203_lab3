---
title: "W203 Lab 3: Reducing Crime by Regression Analysis"
author: "Thomas Drage, Venkatesh Nagapudi, Miguel Jaime"
date: "November 2018"
output: 
  pdf_document: 
    fig_height: 3
    fig_width: 4
    keep_tex: yes
fontsize: 10pt
classoption: a4paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
suppressPackageStartupMessages(library(car))
suppressPackageStartupMessages(library(stargazer))
suppressPackageStartupMessages(library(lmtest))
suppressPackageStartupMessages(library(sandwich))
suppressPackageStartupMessages(library(lm.beta))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(corrplot))
```

#1. Introduction

This statistical investigation aims to understand the determinants of crime to suggest policies to the local government. The study is based upon development of causal models for crime rate, based on county level demographic and judicial data for 1987. We identified factors which modify the rate and extended this to the development of policy proposals for the incoming administration.

\tableofcontents


\pagebreak

# 2. Review of Source Data

```{r}
rm(list = ls())
crime_data = read.csv("crime_v2.csv")
objects(crime_data)
```

Overview of type and number of observations:
```{r}
str(crime_data)
```
There are 97 records in total.

## Data Cleansing

Initially, we examined the data and removed values which were measurement or recording errors and ensured the formatting of the dataset was consistent and able to be processed.

1. We noticed six rows with no data, all fields were "NA" data. Proceeded to remove these rows, since they are most likely an import error, and contain no data that could be analyzed. 
```{r}
crime_data[!complete.cases(crime_data), 1:3]
crime_data_corr = na.omit(crime_data)
```

2. Due to the presence of a random back-tick character in the now removed "NA" rows at the end of the dataset, the Variable prbconv was interpreted as a factor of levels - we can convert it back to numeric data with no loss.
```{r}
crime_data_corr$prbconv_fix = as.numeric(as.character(crime_data_corr$prbconv))
summary(crime_data_corr$prbconv_fix)
```

3. Probability values are greater than one in some cases for probability of arrest, and probability of conviciton. Probability of prison time does not exhibit this behavior. However, having an event with probability greater than one does not make sense: there cannot be a probability value higher than "certain to occur". 

If we assume the probability of arrest is the number of arrests or number of convictions dividied by the number of offenses, it is plausible that a given offense was committed by more than one individual. In these cases, there could be more than one arrest or conviction for a single offense. The variable, then, overestimates the probability of being arrested or convicted for a given offense. This issue could be present on all observations, not just on the ones where the justice system secured enough arrests or convictions to have the varable in question be greater than one. 

Removing the variables would remove from our analysis certain counties would not fix the potential overestimation, it would simply remove from our analysis counties that seem to have better-than-average arrest or conviction rates. In light of this, we decided to include the rows in our analysis. We elected not top code these as one either, since we would be artificially lowering their numbers while leaving other overstimations intact. 

```{r}
crime_data_corr[crime_data_corr$prbarr > 1, "prbarr"]
crime_data_corr[crime_data_corr$prbconv_fix > 1, "prbconv_fix"]
crime_data_corr[crime_data_corr$prbpris > 1, "prbpris"]
```


4. There is a duplicate entry for county #193. We verified that all the data was the same, including the county, and once confirmed we removed the observation from the dataset.

```{r}
crime_data_corr[crime_data_corr$county == 193, 1:6]
crime_data_corr2 = crime_data_corr[!duplicated(crime_data_corr), ]
```

5. There is a density value of 0.0002 - this is approximately one person in an area the size of Alabama and presumably a measurement error. Therefore, we removed this record from the dataset.

```{r}
good_density = (crime_data_corr2$density > 0.001)
crime_data_corr3 = subset(crime_data_corr2, good_density)
```

6.

During EDA, whilst investigating the relationship of prbarr and prbconv_fix in detail we noted an extreme outlier:
```{r}
plot(prbarr ~ prbconv_fix, data = crime_data_corr3)
```
The value at prbconv = 1.5 and prb_arr = 1.1 is most likely a manual recording to cope with missing data; we note that all other probability values are specified to six significant figures, however, for this record prbarr and prbpris were 1.5 and 0.5 respectively (see below). We also found that keeping this value resulted in a leverage point with high Cook's distance in our models. On the basis that this data is a poor manually entered estimate we chose to remove this row from the dataset.

```{r}
crime_data_corr3[51,]
```

```{r}
crime_data_corr4 = crime_data_corr3[-c(51),]
```

After cleansing we have 88 records, which we store as our master dataset:
```{r}
crime_data_clean = crime_data_corr4
```

\pagebreak

# 3. Identification of Key Variables

## Dependent Variable

Crime rate ("crmrte") is the key dependent variable in this study and represents the number of crimes committed per person in each county. 

Summarizing the variable we note a small range of fractional values, centred on a mean of approximately 3.5 crimes per hundred people in the year period.
```{r}
summary(crime_data_clean$crmrte)
```

The distribution of crime rate is right-skewed in this dataset. 

Even though the number of observations (89) is large enough for modelling without concern for the skew noted in the variable. In the data transformation section we will determine if a transformation is needed for separate reasons. 
```{r}
hist(crime_data_clean$crmrte, breaks = 30,
     main = 'Histogram of Crime Rate',
     xlab = 'Crime Rate', cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9)
```

We can take an immediate view of this dependents correlation with our set of available indepedent variables, firstly excluding the wage variables:

```{R}
corrplot(cor(crime_data_clean[, c(3,4,26,6,7,8,9,10,14,24,25)]), method="ellipse", type="upper", tl.col="black", tl.srt=45)
```

We note the cases with particularly high correlation with crmrte and examine them below as candidate regression variables. Note that due to the sample size and desire to maintain applicability of the Central Limit Theorem as well as limited definition of the differences, sizes or boundaries of these geographic regions, we have not partitioned our dataset using the "central"/"west"/"urban" variables.

## Independent Variables - Judicial

1. Probablity of Arrest ("prbarr")
2. Probability of Conviction ("prbconv")
3. Probability of Going to Prison ("prbpris")
4. Average Sentence ("avgsen")

It is likely that crime rate will be lower when the probability of getting arrested, convicted or going to prison is higher due to the deterrent effect. These variables are expected to have causal relationships with crime rate ("crmrte") and should reveal correlation, which we examine through a scatterplot matrix:

```{r fig.height=4, fig.width=5, fig.align='center'}
scatterplotMatrix(~ crmrte + prbarr +  prbconv_fix + prbpris, data=crime_data_clean)
```

The crime rate ("crmte") is negatively correlated with the probability of arrest ("prbarr") and probability of conviction ("prbconv_fix"), which is intuitive. There is perhaps a positive correlation to prbpris, the probability of prison sentencing, which is not intuitive, but the direction of the correlation is not clear from the dataset, therefore we excluded this from our key variable set.

In the correlation matrix above we noted little correlation for the average sentence ("avgsen")" with crime rate:
```{r}
summary(crime_data_clean$avgsen)
cor(crime_data_clean$crmrte,crime_data_clean$avgsen )
```
There is a small correlation, but it is unclear as to whether there will be a causal relationship and which way it would be directed. For this reason we discount this variable from our analysis. 

```{r fig.height=4, fig.width=5, fig.align='center'}
plot(crime_data_clean$avgsen, crime_data_clean$crmrte)
```

## Independent Variables - Demographic
1. Police per capita ("polpc")
2. Density ("density")
3. Tax revenue per capita ("taxpc")
4. Percentage of Young males ("pctymle")
5. Percentage of minorities ("pctmin80")

The second set of independent variables are demographic factors which may lead to changes in crime rate, typically in relation to the affluence of the county. Given that the data is collected at county level, these represent an average and any one county may contain a mix of areas (urban/suburban, wealthy/low-income) with corresponding variations in demographics and crimes, which are not captured in this dataset.

## Policing / Density / Tax Revenue

We examined the effect of police staffing, population density and tax revenue:
```{r fig.height=4.3, fig.width=5.5, fig.align='center'}
scatterplotMatrix(~ crmrte + polpc +  density + taxpc, data=crime_data_clean)
```

Crime Rate is positively correlated to police per capita. However, because of this we consider police staffing a lagging indicator: where crime rate is high, more police officers are deployed. There does not appear to be a logical causality where deployment of police leads to greater crime, but this is one of the strongest correlations revealed.

Looking at population density, there is a positive correlation between crime and density. This is not unexpected given high density housing is often associated with lower incomes and, in some cases, social issues. The density distribution is not normal, and might need to be transformed.

```{r}
summary(crime_data_clean$density)
cor(crime_data_clean$crmrte,crime_data_clean$density)
```

Tax revenue per capita ("taxpc" ) can be considered a proxy for the income level of a county. We assume that the higher the tax paid the more likely that the people are, on average, more wealthy. Wealthier counties might be a more attractive target for property crime; though these effects might be tempered by a higher opportunity cost for committing crime, and higher likelihood of having higher security measures (such as alarms, gated communities, etc.)

```{r}
summary(crime_data_clean$taxpc)
cor(crime_data_clean$crmrte, crime_data_clean$taxpc)
```

We see a positive correlation between "taxpc" and crime rate. The distribution of "taxpc" is not optimal and we may need to examine outliers closely if this is to be used in modelling.

```{r}
hist(crime_data_clean$taxpc, breaks = 50,
     main = 'Histogram of Tax Revenue Per Capita',
     xlab = 'Tax Revenue Per Capita', cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9)
```

## Wage Data

A number of variables are provided with average wages in various sectors in each county. We can first examine these to see which might be correlated with crime rate or our other key variables.

We initially noted an anomaly with the service industry wage (wser) variable, in that the correlation had the opposite sign. However, it was found that a single value greatly exceeded the normal range of values (see below). The figure is likely a result of decimal place recording error as no other remarkable features are found in this county and a service industry wage this high is abnormal, for the purpose of investigating wage data, this row was temporarily removed.
```{R}
summary(crime_data_clean$wser)
crime_data_clean3 = subset(crime_data_clean, crime_data_clean$wser < 1000)
```
We then examine a correlation matrix:
```{R}
corr_w = cor(crime_data_clean3[, c(3,4,26,6,7,8,9,10,14,25,15,16,17,18,19,20,21,22,23)])
corrplot(corr_w[seq(1,10),seq(11,19)], tl.col = "black")
```

The first observation is that all of the wage factors are correlated postively with crime rate. This is quite interesting as one might have assumed that areas where people are paid less would be poorer and would be prone to greater crime. This is apparently untrue, most likely because the comparative average wage in each sector is more of a function of the competitiveness of the economy in the county, evidenced by the strong correlation with density. E.g. a person in a particular industry may make more when employed in a city, which for other social reasons has a higher crime rate than a rural area. Incidentally, this data may also not capture the nature of poverty because it appears to be the average wage of the *employed* in this industry and gives no indication of the proportion of unemployed and hence pooer or criminally employed people in the county.

Another possibility is that crime is logged based on the county where it is committed, not the county where the offender reside. This would further assume that a non-neglibigble number of criminals would live in a county with a lower average wage, and go on to commit crime in a nearby, better-off county where victims would be more likely to possess valuables worth stealing. This effect would manifest primarily in property crimes, and the type of crime is unfortunately not avaiable in this dataset.   

An unfortunate correlation is that of the presence of minorities with the service wage - a high proportion of minority residents appears to push down the service wage, possibly due to competition for such jobs. However, a higher service wage does potentially decrease the probability of arrest. As this effect, while useful, appears to be confounded by the density effect, we do not choose to include such variables in our regression.

## Minorities and Young Males

Here we examine the relationship between the proportion of young males ("pctymle") and the percentage of minority population ("pctmin80") with crime rate:

```{r fig.height=4, fig.width=5, fig.align='center'}
scatterplotMatrix(~ crmrte + pctymle +  pctmin80, data=crime_data_clean)
```

The crime rate is higher in places with a higher percentage of young males. The crime rate is also higher when the percentage of minority population is higher. Both variables seem to have non-ideal distributions.

Looking at the correlation between the variables:
```{r}
summary(crime_data_clean$pctymle)
cor(crime_data_clean$crmrte,crime_data_clean$pctymle)
summary(crime_data_clean$pctmin80)
cor(crime_data_clean$crmrte,crime_data_clean$pctmin80)
```
The correlation is weak in both cases.
\pagebreak

# 3. Data Transformation

## Crime Rate

As discussed in section 2, our main variable of interest, crime rate, is measured in a way that results in small variations between values and a skewed distribution. The histogram below shows this distribution.

```{r fig.width=3, fig.show='hold', fig.align='center'}
hist(crime_data_clean$crmrte, breaks = 30,
     main = 'Histogram of Crime Rate',
     xlab = 'Crime Rate', cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9)

crime_data_clean['log_crmrte'] = log(crime_data_clean$crmrte)
hist(crime_data_clean$log_crmrte, breaks = 30,
     main = 'Histogram of log(Crime Rate)',
     xlab = 'log(Crime Rate)', cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9)
```

We applied a logarithmic transformation, as shown above to the variable which addresses both issues well. 

This transformation will change our interpretation, since the model coefficients will represent percentage changes for crime rate. Given the intended usage in reducing this rate and small values of the variable in its original units, this change will make the results easier to interpret.

## Density 

Density is right-skewed, however, the variable becomes more normal if we apply a log transformation, as shown below.

```{r fig.width=3, fig.show='hold', fig.align='center'}
hist(crime_data_clean$density, breaks = 30,
     main = 'Histogram of Density',
     xlab = 'Density', cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9)

hist(log(crime_data_clean$density), breaks = 30,
     main = 'Histogram of Density',
     xlab = 'Density', cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9)
```

Previously we noted that this variable has high correlation with our target variable, which increases slightly with the log transformation. The effect of removing this non-linearity is quite visible in the plot below.

```{R  fig.height=4, fig.width=5, fig.align='center'}
cor(crime_data_clean$log_crmrte,crime_data_clean$density)
cor(crime_data_clean$log_crmrte,log(crime_data_clean$density))

scatterplotMatrix(~ log_crmrte + density + log(density), data = crime_data_clean)
```

For this reason, we establish a transformed variable for use in our analysis:

```{R}
crime_data_clean['log_density'] = log(crime_data_clean$density)
```

## Police per Capita

The normality and correlation of the "polpc" variable also benefit from logarithmic transformation, with an improvement in normality and correlation. The plot below shows an improvement in the linearity with the logarithm of crime rate.
```{R  fig.height=3, fig.width=5, fig.align='center'}
cor(crime_data_clean$log_crmrte,crime_data_clean$polpc)
cor(crime_data_clean$log_crmrte,log(crime_data_clean$polpc))
scatterplotMatrix(~ log_crmrte + polpc + log(polpc), data = crime_data_clean)
```

We can define a new transformed variable, however we note that this quantity is still believe to be a lagging indicator and potentially unsuitable for causal modelling.
```{R}
crime_data_clean['log_polpc'] = log(crime_data_clean$polpc)
```

\pagebreak

# 4. Regression Modelling


## Model 1 - minimal using the Judicial system variables only

```{R}
model1 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr + 
              crime_data_clean$prbconv_fix 
              )
model1$coefficients
```

Our hypothesis underlying this simple model is that the crime rate is correlated with the efficiency of the justice system, all other demographic factors being approximately equal as justice deters and controls the proliferation of criminal activity. The negative coefficients above support this with the probability of arrest being a stronger contributor than the probability of conviction.

However, this is not the most complete model, and the R^2^ value is relatively poor and reveals scope for a more sophisticated model:
```{R}
summary(model1)$r.square
```

We can then plot diagnostics for Model 1 to evaluate OLS assumptions:
```{R fig.width=3.1, fig.height=4, fig.show='hold', fig.align='center'}
plot(model1, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 1)
plot(model1, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 3)
```

We do note two issues:

* *MLR4 Zero Conditional Mean*: The fitted vs. residuals plot above shows a violation of zero-conditional mean for this model. This suggests some non-linearity with our chosen indepedents.
* *MLR5 Homeskedasticity*: This model is heteroskedastic. We confirm this using the  Breusch-Pagan test and note marginal confirmation. For this reason, we will use robust standard errors going forward.

```{R}
bptest(model1)
```

We check the other assumptions anyway in order to gain insight into issues with this variable selection:

```{R fig.width=3.1, fig.height=4, fig.show='hold', fig.align='center'}
plot(model1, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 2)
plot(model1, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 5)
```
* *MLR6 Normality*: The Q-Q plot indicates a good normality of the residuals. 

There are no points with Cook's distance > 1 indicating that we removed all the outliers earlier on in the analysis.

```{R}
model1 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr + 
              crime_data_clean$prbconv_fix 
              )
model1$coefficients
```

## Model 3 - using judicial and demographic system variables

Model 3 is a more elaborate model that takes into account both judicial and demographic system variables to come up with a better causal explanation of crime rate. In this model, we included all meaningful variables. We decided to leave out wage-related variables since we did not find them to be relevant to our analysis.

```{r}
model3 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr + 
              crime_data_clean$prbconv_fix + 
              crime_data_clean$prbpris + 
              crime_data_clean$avgsen + 
              crime_data_clean$log_polpc +
              crime_data_clean$log_density + 
              crime_data_clean$taxpc + 
              crime_data_clean$pctmin80 + 
              crime_data_clean$pctymle)
```


We can then plot Model 3 to evaluate OLS assumptions:
```{R fig.width=3.1, fig.height=4, fig.show='hold', fig.align='center'}
plot(model3, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 1)
plot(model3, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 3)
```

With this model, we note significant improvement:

* *MLR4 Zero Conditional Mean*: The inclusion of more explanatory variables improves the mean residual, making it closer to zero.
* *MLR5 Homoskedasticity*: This model appears to have improved the scale location plot and the Breusch-Pagan test does not reject homoskedasticity.
```{R}
bptest(model3)
```

```{R fig.width=3.1, fig.height=4, fig.show='hold', fig.align='center'}
plot(model3, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 2)
plot(model3, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 5)
```
* *MLR6 Normality*: The Q-Q plot shows some deviations from normality which we confirm with a Shapiro-Wilks test. This suggests non-linearity in one or more of our model variables, most prominently in lower quartiles.
```{R}
shapiro.test(model3$residuals)
```

However, we note there are a couple of high influence outliers which may be negatively affecting the regression.

We can then display Model 3 using heteroskedastic robust standard errors:

```{r results="asis"}

# Using robust errors to compensate for heteroskedasticity
robust_se <- function(model) {
  cov <- vcovHC(model)
  sqrt(diag(cov))
}

robust_errors <- list(robust_se(model3))

stargazer(model1, model3,
          star.cutoffs =c(0.05,0.01, 0.001),
          se = robust_errors,
          type = 'latex',
          column.labels = c('Model 1','Model 3'),
          font.size = 'small',
          float = FALSE)

```

We can also evaluate the Akaike Information Criterion for both models to check goodness of fit relative to parsimony:
```{r}
AIC(model1, model3)
```

As we can see, Model 3 has significanlty improved on the AIC, R2 and Residual SE, but there are some coefficients which are not statistically significant (prbconv_fix, polpc). There is likely a more optimized model that has fewer coefficients that we can derive out of the Model 1 and Model 3 experiments above.

By looking at the standardised co-efficients, we can evaluate compare the effect of changes of each variable on the crime rate:
```{R}
lm.beta(model3)
```
Based on the above we may consider removing those with lower (e.g. <0.1) gain in addition to those which are not statistically significant. 

## Model 2 - with optimized Judicial and Demographic system variables

From the above models, it is clear that some of the variables added to the model such as density, police per capita ("polpc") and proportion of minorities ("pctmin80") show particularly strong contribution to the model. Whilst still statistically significant and one of our strongest gain variables, the prbconv_fix coefficient has decreased in magnitude in Model 3. This is likely due to correlation with density and complex relationships which are not revealed with the available variables (see ommitted variable discussion below). 

We previously noted that we do not believe police per capita ("polpc") to indicate causality, merely correlation and so we attempt the remove of this from the model. Examining standardised coefficients below we note that the removal of "polpc" has increased the effect size of variables such as tax per capita ("taxpc") and proportion of young males ("pctymle") as well as density. We therefore consider this variable to have confounded these individual effects being a product of crime and affluence in the community. For this reason, as we seek a causal model for crime rate, we chose not to include this variable in our next model.

```{r}
#everything in model3 except polpc
model3_no_polpc = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr + 
              crime_data_clean$prbconv_fix + 
              crime_data_clean$prbpris + 
              crime_data_clean$avgsen + 
              #crime_data_clean$polpc + 
              #crime_data_clean$log_polpc +
              crime_data_clean$log_density + 
              crime_data_clean$taxpc + 
              crime_data_clean$pctmin80 + 
              crime_data_clean$pctymle)
lm.beta(model3_no_polpc)
```

Further to this, we have chosen to remove average sentence length ("avgsen"), probability of imprisonment ("prbpris") and proportion of young males ("pctymle") due to the high variance associated with the coefficient and low effect size. We therefore select the following for our second model, subject to further statistical testing:

1. prbarr

2. log(density)

3. pctmin80

4. prbconv_fix

5. taxpc

Creating Model 2 out of these variables:

```{r}
model2_ver1 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr +  
              crime_data_clean$log_density +  
              crime_data_clean$prbconv_fix +
              crime_data_clean$taxpc + 
              crime_data_clean$pctmin80)
coeftest(model2_ver1, vcov = vcovHC)
```
We can see that t-testing indicates that tax per capita ("taxpc") is not statistically significant, as a result we will remove it from the final model and retain the other four variables to get to a more parsimonious model.


```{r}
model2 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr +  
              crime_data_clean$log_density +  
              crime_data_clean$prbconv_fix +
              crime_data_clean$pctmin80)
```

## Detailed Verification of OLS Assumptions

We can then plot diagnostics for Model 2 to evaluate OLS assumptions:
```{R fig.width=3.1, fig.height=4, fig.show='hold', fig.align='center'}
plot(model2, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 1)
plot(model2, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 3)
```

* *MLR1 Linearity*: The model is linear in the parameters given.
* *MLR2 Random Sampling*: The sampling process is not clear but assumed to be a random selection of counties.
* *MLR3 Colinearity*: Inspection of scatterplots above did not reveal any perfect colinearity amongst the chosen varaibles.
* *MLR4 Zero Conditional Mean*: Further improvement and no violation of zero conditional mean in this model.
* *MLR5 Homoskedasticity*: This model appears to have futher improved the scale location plot and the Breusch-Pagan test does not reject homoskedasticity.
```{R}
bptest(model2)
```

```{R fig.width=3.1, fig.height=4, fig.show='hold', fig.align='center'}
plot(model2, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 2)
plot(model2, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub	= 0.5, which = 5)
```
* *MLR6 Normality*: The Q-Q plot is more normal than Model 3. So the coefficients are more robust. We see that the p-values are all very significant unlike Model 3's p-values showing that the coefficients are more consistent. The Shapiro-Wilks test does not reject normality:
```{R}
shapiro.test(model2$residuals)
```

Additionally, there are no outliers with high influence in this model specification.

## Comparison of Models

We can now compare all three models. We used the "model3_no_polpc" so that we can do an "apples to apples"" comparison with Model 2 by comparing selection of causal variables to indicate the robustness of our final model.

```{r results="asis"}

robust_errors <- list(robust_se(model1),
                      robust_se(model2),
                      robust_se(model3))

stargazer(model1, model2, model3_no_polpc,
          star.cutoffs =c(0.05,0.01, 0.001),
          se = robust_errors,
          type = 'latex',
          column.labels = c('Model 1', 'Model 2', 'Model 3 No polpc'),
          font.size = 'small',
          float = FALSE)
```

Additionally, we may compare the models in terms of Aikaike's Information Criterion:

```{r}
AIC(model1, model2, model3_no_polpc)
```

Let's also compare the BIC in these 3 cases:
```{r}
BIC(model1, model2, model3_no_polpc)
```

Model 2 shows significant improvement over Model1 with a better AIC, much better R^2^ and lower Residual SE. We note that the F-test supports all three models, but our chosen model has a greater statistic, suggesting strong joint significance of our model. Additionally, the BIC (another goodness of fit measure which penalises addition of variables) for Model 2 is better than Model 3 as well which makes a very strong case for Model 2 as the superior model..

\pagebreak

#5. Discussion 

## Model Specification & Omitted Variables

It is likely that crime rate will be heavily influenced by the following omitted variables.


Variable Category  | 
------------------ | ----------------------------------------------------------------
Demographics       | The dataset contains little information about demographics. The pctmin80 variable, for example, is based on dated information about minorities, and it does not break down into proportions for particular minorities. Updated, more granular information could help model crime rate. Other demographics variables, such as religious make up, could also be relevant to our analysis due to their potential impact on criminal activity.
                   |
Education Level    | We expect higher education levels to be correlated with lower crime rates, and it could lead to interesting policy recommendations, but that information is unfortunately not available in the datset. 
                   |
Employment         | We assume employment rate to be negatively correlated with the crime rate, but the dataset does not contain employment-related information. The e considered using wage information as a proxy, under the reasoning that lower employment would result in upward pressure on wages. Scarcity of labor is, however, only one of many factors affecting wages, which means the proxy could be significantly biased. The absence of employment data might bias our models by attributing high crime rates to other factors, when lack of job prospects could be the actual cause. 
                   |
Land Use           | Crime rates might be correlated with land use planning practices. We expect, for example, areas with with vibrant nightlife and rural ares to experience different rates and types of crimes. This omitted variable would potentially bias our density dependance by increasing the co-efficient or even increase the dependence of our model probability of arrest due to police presence in certain urban areas.
                   |
Age Distribution   | The only variable provided was the percent of the population who are young males. It would be useful to have similar information about other age groups as well. We expect groupslike young children and seniors to have a negative effect on crime rate. Both of these groups typically increase density, but decrease crime and we would expect their presence to bias the density variable negatively.
                   |
Crime Information  | Different types of crime (violent crime, property crime, etc.) might be correlated with different variables. Omitting this variable could bias the probability of arrest, conviction and average sentence. Sentence length, for example, would be dependant on a measure of the proportion of non-violent crime and we may find sentence length is also negatively biased by this omission.
                   |
Wages              | We presuppose higher wages affect the risk equation for committing crimes, but increasing the cost (if caught), and lowering the relative value of the rewards. The dataset does include certain wage information, though we found their effects to run counter to our expectations, and it had high correlation with density. Perhaps more information regarding wages (such as quartiles) and employment data could help understand this effect better.

## Practical Significance of Causal Model

Based on our analysis, the probability of arrest and conviction help drive down crime rates. Increasing the probability of arrest by one unit is correlated with a 181% decrease in crime rate. Increasing the probability of conviction by one unit is correlated with a 61% decrease in crime rate. 

Density is positively correlated with crime rate. An increase of one unit in density is correlated with a 36% increase in the crime rate.

#6. Conclusions

Based on these results of our regression, we have a better understanding of causal factors that lead to an increase or decrease in the crime rate. Based on our analsys, we make the following recommendations:

1. Fear of punishment

Our analysis shows that the higher the probability of arrest and conviction, the lower the crime rate. The fear of punishment is definitely a very effective tool to combat crime. As such, we recommend that the policy makers increase awareness of the effectiveness of the judicial system to reduce crime.Bringing pepetrators to justice with an effective police force could reduce overall crime rates significantly.

2. Crime watching in densely populated areas

Our analysis shows that in densely populated areas, the crime rate tends to be much higher. There coud be several factors at play here including demographics, the density of police force, the earned wages and so on. In particular, policy makers should ensure that there is a larger active police force in densely populated areas to battle crime more effectively.

3. Better education and employment

While the regression data does not include the effects of better education and employment on crime rate, there are some indications that the presence of more minorities and young males results in higher crime rates. Policy makers can focus on better education amongst youth and minorities so that the resulting higher employment rates lead to lower crime rates


