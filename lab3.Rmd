---
title: "lab3: Reducing Crime"
author: "Thomas Drage, Venkatesh Nagapudi, Miguel Jamie"
date: "November 20, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(library(car))
suppressPackageStartupMessages(library(stargazer))
suppressPackageStartupMessages(library(lmtest))
suppressPackageStartupMessages(library(sandwich))
```

#1. Introduction

This statistical investigation is aimed at understanding the determinants of crime in order to generate policy suggestions that are applicable to the local government. The study is based upon development of causal models for crime rate, based on county level demographic and judicial data for 1987. We have identified factors which modify the rate and extended this to the development of policy proposals for a new government.

## What we are graded on:
Introduction. As you understand it, what is the motivation for this team's report? Does the introduction as written make the motivation easy to understand? Is the analysis well-motivated? Note that we're not necessarily expecting a long introduction. Even a single paragraph is probably enough for most reports.

# 2. Review of Source Data

```{r}
rm(list = ls())
crime_data = read.csv("crime_v2.csv")
objects(crime_data)
```

Finding out number of observations
```{r}
str(crime_data)
```
There are 97 of them.

## Data Cleansing

1. Removing NA in some cases
```{r}
crime_data_corr = na.omit(crime_data)
```

2. Some values are coded as levels: prbconv - need to fix
```{r}
crime_data_corr$prbconv_fix = as.numeric(as.character(crime_data_corr$prbconv))
summary(crime_data_corr$prbconv_fix)
```

3. Probability values > 1 in some cases. There are 11 such values. Perhaps we have to leave these rows out.
```{r}
sum(crime_data_corr$prbarr > 1)
sum(crime_data_corr$prbconv_fix > 1)
sum(crime_data_corr$prbpris > 1)
```

Eliminate the above points from the data set
```{r}
good_prob_cond =
   !((crime_data_corr$prbarr > 1) | 
   (crime_data_corr$prbconv_fix > 1) |
   (crime_data_corr$prbpris > 1))
crime_data_corr2 = subset (crime_data_corr, good_prob_cond)
str(crime_data_corr2)
```

4. There is a duplicate entry for county #193, which we will also remove from the data set.

```{r}
crime_data_corr2[crime_data_corr2$county == 193, 1:6]
crime_data_corr3 = crime_data_corr2[!duplicated(crime_data_corr2), ]
```

Now there are only 80 observerations which is less than 100. So we do need to be careful our assumptions around CLM for coefficients being normal.

Once data cleaning is complete, creating a working copy of our data
```{r}
crime_data_clean = crime_data_corr3
```

# 3. Identification of Key Variables

### Dependent Variable

The crime rate ("crmrte") is the key dependent variable in this study and represents the number of crimes committed per person in the each county. 

Summarizing the variable we note a small range of fractional values, centred on a mean of approximately 3.5 crimes per hundred people in the year period.
```{r}
summary(crime_data_clean$crmrte)
```

The distribution of crime rate is somewhat left-skewed in this dataset but sufficient data is available for modelling.
```{r}
hist(crime_data_clean$crmrte, breaks = 30,
     main = 'Histogram of Crime Rate',
     xlab = 'Crime Rate' )
```


### Independent Variables - Judicial

1. Probablity of Arrest ("prbarr")
2. Probability of Conviction ("prbconv")
3. Probability of Going to Prison ("prbpris")

The assumption here is that crime rate will be lower if the probability of getting arrested, convicted or going to prison is higher. Crimes happen if criminals believe that they can get away with performing crimal acts since the probability of getting punished is lower.

Let's look at the scatterplot matrix for a relationship with crmrte
```{r}
scatterplotMatrix(~ log(crmrte) + prbarr +  prbconv_fix + prbpris, data=crime_data_clean)
```
As we can see, the log(crimrte) seems to be negatively correlated with prbarr and prbconv_fix which seems to be intuitive. There is perhaps a positive correlation to prbpris, which is not very intuitive. 

But on the flipside, what really motivates or prevents crime? Should we have some of those variables in here as primarly independent variables?

### Idependent Variables - Demographic
Should some of these be key independent variables?
1. Police per capita ("polpc")
2. Density ("density")
3. Tax revenue per capita ("taxpc")
4. Percentage of Young males ("pctymle")
5. Percentage of minorities ("pctmin80")
6. Average sentence ("avgsen")

Crime rate likely depends on the deterrents to crime: police protection
But it is likely crime is high if the county is poor or has young males/minorities (ex: Oakland?) but not when the county is rich (there are people to rob, but then there will be better protection as well like security alarms etc).

Performing a couple of different scatterplots
```{r}
scatterplotMatrix(~ log(crmrte) + polpc +  density + taxpc, data=crime_data_clean)
```
Crime Rate seems to be positively correlated to the "Police per capita". If we consider police staffing as a lagging indicator, this is intuitive: where Crime Rate is high, more police officers will be deployed. This would be an inverse causal relationship.

Looking at population density, there is a positive correlation between crime and density. This seems intuitive. However, the density distribution is not very normal, and might need a transformation.

```{r}
summary(crime_data_clean$density)
cor(crime_data_clean$crmrte,crime_data_clean$density)
```
```{r}
hist(crime_data_clean$density, breaks = 50,
     main = 'Histogram of Density',
     xlab = 'Density' )
```

The Taxpc is a proxy for how rich a county is. It is likely that the higher the tax paid, the more likely that the people are, on average, richer. On one hand, richer counties might be a more attractive target for property crime. On the other hand, people in this counties have less of an economic incentive to commit crime, and are likely to have better security measures than less rich counties.

```{r}
summary(crime_data_clean$taxpc)
cor(crime_data_clean$crmrte, crime_data_clean$taxpc)
```

Look at the correlation, we see a positive correlation between taxpc and crime rate. However, the distribution of taxpc is not very optimal and we do need to watch out for outliers creating a lot of leverage and influence.

```{r}
hist(crime_data_clean$taxpc, breaks = 50,
     main = 'Histogram of Tax Revenue Per Capita',
     xlab = 'Tax Revenue Per Capita' )
```

Examining the relationship between pctymle and pctmin80 with crmrte:

```{r}
scatterplotMatrix(~ log(crmrte) + pctymle +  pctmin80, data=crime_data_clean)
```

The crime rate is higher in places with more % of young males. This seems somewhat likely.  The crime rate is higher generally when minority % is higher. However, both variables seem to have non-ideal distributions.

(NOTE(miguel): seemscorrelation is not very high, but it might still be important, IMO.)

Looking at the correlation between the variables:
```{r}
summary(crime_data_clean$pctymle)
cor(crime_data_clean$crmrte,crime_data_clean$pctymle)
summary(crime_data_clean$pctmin80)
cor(crime_data_clean$crmrte,crime_data_clean$pctmin80)
```
The correlation is not very high though.

Finally looking at avgsen,
```{r}
summary(crime_data_clean$avgsen)
cor(crime_data_clean$crmrte,crime_data_clean$avgsen )
```
There is a small correlation here. But it is unclear as to whether there will be a causal relationship and which way it would be directed. 

```{r}
scatterplotMatrix(~ log(crmrte) + avgsen, data=crime_data_clean)
```

# 3. Data Transformation
Not sure if we have transformations in this section or in the later models section:

2. What transformations should you apply to each variable? This is very important because transformations can reveal linearities in the data, make our results relevant, or help us meet model assumptions.

Some inputs from today's post-class session:
1. Use a log transformation on crime rate since the values are very small
2. Apply transformations in X variables and try to figure out if r.square improves or MSE goes down (this requires a model to be built though)
3. There was a discussion on Y-transformation which I didn't understand at all...not sure what that is...perhaps week 12 async has it?
4. If you apply Y-transformation, apply it universally (Prof said this: not sure what it means!)

## Crime Rate

As discussed in section 2, our main variable of interest, crime rate, is measured in a way that results in small variations between values, and a skewed distribution:

```{r}
summary(crime_data_clean$crmrte)
hist(crime_data_clean$crmrte, breaks = 50,
     main = 'Histogram of Crime Rate',
     xlab = 'Crime Rate' )
```

As a result, we will apply a log() transformation to the variable, which will address both issues. 

This transformation will change our interpretation, since the model results will be for percentage changes for Crime Rate. Given the small values of the variable in its original units, this change in interpretation will make the results easier to interpret. 

```{r}
crime_data_clean['log_crmrte'] = log(crime_data_clean$crmrte)
summary(crime_data_clean$log_crmrte)
hist(crime_data_clean$log_crmrte, breaks = 50,
     main = 'Histogram of log(Crime Rate)',
     xlab = 'log(Crime Rate)' )
```


# 4. Regression Modelling


## Model 1 - using the Judicial system variables only

```{R, results='asis'}
model1 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr + 
              crime_data_clean$prbconv_fix + 
              crime_data_clean$prbpris + 
              crime_data_clean$avgsen)
model1
```

Plotting the model1 to look at heteroskedasticity, zero conditional mean violation and so on:
```{r}
plot(model1)
```
Zero conditional mean is violated. The Q-Q plot indicates a good amount of normality. This model is defintiely heteroskedastic from looking at the scalel-location plot.There are no points with Cook's distance > 1 which means that there are no significant outliers.

## Model3: With all variables

```{r}
model3 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr + 
              crime_data_clean$prbconv_fix + 
              crime_data_clean$prbpris + 
              crime_data_clean$avgsen + 
              crime_data_clean$polpc + 
              crime_data_clean$density + 
              crime_data_clean$taxpc + 
              crime_data_clean$pctmin80 + 
              crime_data_clean$pctymle)
```


```{r}
stargazer(model1, model3, star.cutoffs = c(0.05,0.01, 0.001), type = "text", float=FALSE)
```

```{R}
AIC(model1, model3)
```

From the above models, it is clear that some of the variables added to the model such as the density, polpc and pctmin80 are definitely improving the model as can be seen above. probconv_fix seems to be getting a lower significance in the model3. Perhaps, it is correlating heavily with other variables and therefore decreasing in significance.

## Model 2

 One model that includes key explanatory variables and only covariates that you believe increase the accuracy of your results without introducing substantial bias (for example, you should not include outcome variables that will absorb some of the causal effect you are interested in). This model should strike a balance between accuracy and parsimony and reflect your best understanding of the determinants of crime.

Let's choose the ones that have the most significance in the stargazer output above. These include:
1. prbarr
2. density
3. pctmin80
4. polpc
Creating model2 out of these variables:

```{r}
model2 = lm(crime_data_clean$log_crmrte ~ 
              crime_data_clean$prbarr +  
              crime_data_clean$density +  
              log(crime_data_clean$polpc) + 
              crime_data_clean$pctmin80)
```

Let's plot the model2 and look at the our assumptions:
```{r}
plot(model2)
```

Let's compare all the models now:


```{r}
# Using robust errors to compensate for heteroskedasticity
robust_se <- function(model) {
  cov <- vcovHC(model)
  sqrt(diag(cov))
}

robust_errors <- list(robust_se(model1),
                      robust_se(model2),
                      robust_se(model3))

stargazer(model1, model2, model3,
          star.cutoffs =c(0.05,0.01, 0.001),
          se = robust_errors,
          type = 'text',
          font.size = 'small',
          float = FALSE)
```

```{r}
AIC(model1, model2, model3)
```

As we can see above, the AIC as well as the standard errors seem to be the best for model2. The addition of more variables really didn't help much as we go from model2 to model3. 

# 5. Discussion - Model Specification & Omitted Variables

Some inputs from class: What we need to discuss is what columns were omitted that could help with getting a better model...

It is likely that crime rate will be heavily influenced by the following omitted variables:
1. Demographics: There is very little information on demographics other than pctmin80 which is based on dated information about minorities. It could be useful to get a bigger idea on the demographics of the county population. 
2. Education level: The higher the education level, the lower the crime rate
3. Wages: The more affluent neighborhoods will tend to have lesser crime. This is somewhat reflected by the tax revenues per capita
4. Private Security: The higher the private security level, the lower the crime rate
5. Number of bars: It's likely that the higher the number of bars in a place, the higher the crime rate is likely to be. This is dependent on "nightlife" - there is a higher probability of crime in places which have a lot of nightlife



## What we need to show:
After your model building process, you should include a substantial discussion of omitted variables. Identify what you think are the 5-10 most important omitted variables that bias results you care about. For each variable, you should estimate what direction the bias is in. If you can argue whether the bias is large or small, that is even better. State whether you have any variables available that may proxy (even imperfectly) for the omitted variable. Pay particular attention to whether each omitted variable bias is towards zero or away from zero. You will use this information to judge whether the effects you find are likely to be real, or whether they might be entirely an artifact of omitted variable bias.

#6. Conclusion

## Appendix
[We can delete this, but moving the code for exp_pris_time here just in case we want to keep it to show our work.]

The data contains several variables related to the potential consequences for a person committing a crime. These are the probabilities of being arrested, convicted, sentenced to prison, and the average length of said sentece. 

Instead of using the variables individually, we will condense them into one, which will incorporate the probabilities of each step as well as the sentence. This variable, which we will call "expected time in prison" or "exp_pris_time", will be obtained by multiplying each probability and the expected average sentence. 
```{r}
crime_data_clean["exp_pris_time"] = crime_data_clean$prbarr * crime_data_clean$prbconv_fix * crime_data_clean$prbpris * crime_data_clean$avgsen
```

The resulting variable is right-skewed, so will then take the log, which yields a more normal distribution, and use that variable going forward. 

```{r}
hist(crime_data_clean$exp_pris_time, breaks = 30,
     main = 'Distribution of Expected Prison Time',
     xlab = 'Expected Prison Time (Days)' )

hist(log(crime_data_clean$exp_pris_time), breaks = 30,
     main = 'Distribution of Log(Expected Prison Time)',
     xlab = 'Expected Prison Time (Days)' )

crime_data_clean["log_exp_pris_time"] = log(crime_data_clean$exp_pris_time)
```

By using both log variables (Crime Rate and Expected Prison Time), we get a less heteroskedastic distribution between our variables, as illustrated by the plots below.

```{r}

lm1 = lm(crime_data_clean$crmrte ~ crime_data_clean$exp_pris_time)
lm2 = lm(crime_data_clean$log_crmrte ~ crime_data_clean$exp_pris_time)
lm3 = lm(crime_data_clean$log_crmrte ~ log(crime_data_clean$exp_pris_time))

paste("Level - Level: ", summary(lm1)$adj.r.squared)
paste("Log - Level:", summary(lm2)$adj.r.squared)
paste('Log - Log:', summary(lm3)$adj.r.squared)

plot(crime_data_clean$exp_pris_time, crime_data_clean$crmrte,
     main = 'Level - Level')
abline(lm1)

plot(crime_data_clean$exp_pris_time, crime_data_clean$log_crmrte,
     main = 'Log - Level')
abline(lm2)

plot(log(crime_data_clean$exp_pris_time), crime_data_clean$log_crmrte,
     main = 'Log - Log')
abline(lm3)

```

